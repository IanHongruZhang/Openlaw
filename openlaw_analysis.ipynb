{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入各类库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引入所有包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from PIL import Image,ImageEnhance\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "from gensim import corpora, models, similarities\n",
    "import codecs, sys\n",
    "import jieba\n",
    "import os\n",
    "import codecs\n",
    "import shutil\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfparser import PDFParser, PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "import tensorflow as tf\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将所有pdf文本，转化为表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pdf转化器\n",
    "def pdfreader(filename):\n",
    "    \"\"\"\n",
    "    将pdf转化为text文本\n",
    "    Param filename:文档名\n",
    "    return final_test:最终文本内容列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #获得文档对象,以二进制读方式打开\n",
    "        fp = open(filename, \"rb\")\n",
    "\n",
    "        #创建一个与文档关联的分析器\n",
    "        parser = PDFParser(fp)\n",
    "\n",
    "        #创建一个pdf文档的对象\n",
    "        doc = PDFDocument()\n",
    "\n",
    "        #连接解释器与文档对象\n",
    "        parser.set_document(doc)\n",
    "        doc.set_parser(parser)\n",
    "\n",
    "        #初始化文档,如果文档有密码，写与此。\n",
    "        doc.initialize(\"\")\n",
    "\n",
    "        #创建pdf资源管理器\n",
    "        resource = PDFResourceManager()\n",
    "\n",
    "        #参数分析器\n",
    "        laparam = LAParams()\n",
    "\n",
    "        #创建聚合器\n",
    "        device = PDFPageAggregator(resource, laparams=laparam)\n",
    "\n",
    "        #创建pdf页面解释器\n",
    "        interpreter = PDFPageInterpreter(resource, device)\n",
    "\n",
    "        #使用文档对象得到页面的集合\n",
    "        list_text,corpus = [],[]\n",
    "        for page in doc.get_pages():\n",
    "            #使用页面解释器读取\n",
    "            interpreter.process_page(page)\n",
    "            #使用聚合器来获得内容\n",
    "            layout = device.get_result()\n",
    "            for out in layout:\n",
    "                if hasattr(out, \"get_text\"):\n",
    "                    line = out.get_text().strip(\"\\n \")\n",
    "                    line_clean = re.sub(\"\\n\",\"\",line)\n",
    "                    list_text.append(line_clean)\n",
    "        final_test = \"\".join([i for i in list_text if i != \"\"])\n",
    "        return final_test\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "os.chdir(\"正当防卫的1000份文书\")\n",
    "file_list = os.listdir()\n",
    "corpus = list(map(pdfreader,file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本清理，提取出所有的重要信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 文本清理器1\n",
    "def clean(corpus_none_n):\n",
    "    \n",
    "    # 正则表达式部分\n",
    "    results = corpus_none_n\n",
    "    ### 原告中的三类可能\n",
    "    pattern_plaintiff = re.compile(r\"原告：([\\u4e00-\\u9fa5].*?)，\") # 原告第一类可能\n",
    "    pattern_plaintiff_2 = re.compile(r\"原告：([\\u4e00-\\u9fa5].*?)。\") # 原告第二类可能\n",
    "    pattern_plaintiff_3 = re.compile(r\"原告.*?，\") # 原告第三类可能\n",
    "\n",
    "    ### 被告中的三类可能\n",
    "    pattern_defender = re.compile(r\"被告：.*?，\")\n",
    "    pattern_defender_2 = re.compile(r\"被告：.*?。\")\n",
    "\n",
    "    ### 原告出生年龄\n",
    "    pattern_plaintiff_birthday = re.compile(r\"原告：.*\\d.*?出生，\")\n",
    "\n",
    "    ### 被告出生年龄\n",
    "    pattern_defender_birthday = re.compile(r\"被告：.*\\d.*?出生，\")\n",
    "\n",
    "    ### 立案时间\n",
    "    pattern_starttime = re.compile(r\"，本院于.*\\d.立案后，\")\n",
    "\n",
    "    ### 诉讼请求\n",
    "    pattern_claims = re.compile(r\"诉讼请求：.*?。\")\n",
    "    pattern_claims_2 = re.compile(r\"诉称，.*本院认为\")\n",
    "\n",
    "    ### 事实和理由\n",
    "    pattern_facts = re.compile(r\"事实和理由：.*。\")\n",
    "\n",
    "    ### 辩护事实\n",
    "    pattern_defence = re.compile(r\"辩称：.*\")\n",
    "    pattern_defence_2 = re.compile(r\"辩称，.*\")\n",
    "\n",
    "    ### 法院认定事实\n",
    "    pattern_court_fact_1 = re.compile(r\"认定事实如下：.*。本院\")\n",
    "    pattern_court_fact_2 = re.compile(r\"经审理查明：.*。本院\")\n",
    "    pattern_court_fact_3 = re.compile(r\"经审理查明，.*。本院\")\n",
    "\n",
    "    ### 法院对事实的判定\n",
    "    pattern_court_advise = re.compile(r\"本院认为.*\")\n",
    "\n",
    "    ### 法院的意见\n",
    "    pattern_court_attitude = re.compile(r\"综上所述.*?。\")\n",
    "\n",
    "    ### 法条\n",
    "    pattern_laws_1 = re.compile(r\"《.*?》.*?条\")\n",
    "    pattern_laws_2 = re.compile(r\"《.*?》.*?条，\")\n",
    "\n",
    "    ### 判决\n",
    "    pattern_judgement = re.compile(\"判决如下:.*。\") #需要使用去掉\\n的文档\n",
    "\n",
    "    ## 搜索原告\n",
    "    def find_plaintiffs(text,pattern1,pattern2,pattern3):\n",
    "        if re.search(pattern1,text):\n",
    "            piece = re.search(pattern1,text).group().strip(\":原告：，\")\n",
    "            return piece\n",
    "        elif re.search(pattern2,text):\n",
    "            piece = re.search(pattern2,text).group().strip(\":原告：，\")\n",
    "            return piece\n",
    "        elif re.search(pattern3,text):\n",
    "            piece = re.search(pattern3,text).group().strip(\":原告：，\")\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 搜索被告\n",
    "    def find_defender(text,pattern1,pattern2):\n",
    "        if re.search(pattern1,text):\n",
    "            piece = re.search(pattern1,text).group().strip(\"被告：\")\n",
    "            return piece\n",
    "        elif re.search(pattern2,text):\n",
    "            piece = re.search(pattern2,text).group().strip(\"被告：\")\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 搜索原告出生年龄\n",
    "    def find_plaintiffs_birthday(text,pattern):\n",
    "        if re.search(pattern,text):\n",
    "            piece = re.search(pattern,text).group().strip(\"，\")\n",
    "            text = re.sub(\"原告：.*，\",\"\",piece).strip(\"出生\")\n",
    "            return text\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 搜索被告出生年龄\n",
    "    def find_defenders_birthday(text,pattern):\n",
    "        if re.search(pattern,text):\n",
    "            piece = re.search(pattern,text).group().strip(\"，\")\n",
    "            text = re.sub(\"被告：.*，\",\"\",piece).strip(\"出生\")\n",
    "            return text\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 立案时间\n",
    "    def find_starttime(text,pattern):\n",
    "        if re.search(pattern,text):\n",
    "            piece = re.search(pattern,text).group().lstrip(\"，本院于\").rstrip(\"立案后，\")\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 事实和理由\n",
    "    def find_facts(text,pattern):\n",
    "        if re.search(pattern,text):\n",
    "            piece = re.search(pattern,text).group()\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 被告辩称\n",
    "    def find_defenders_words(text,pattern1,pattern2):\n",
    "        if re.search(pattern1,text):\n",
    "            piece = re.search(pattern1,text).group()\n",
    "            return piece\n",
    "        elif re.search(pattern2,text):\n",
    "            piece = re.search(pattern2,text).group()\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 法院认定事实\n",
    "    def find_court_facts(text,pattern1,pattern2,pattern3):\n",
    "        if re.search(pattern1,text):\n",
    "            piece = re.search(pattern1,text).group().rstrip(\"本院\")\n",
    "            return piece\n",
    "        elif re.search(pattern2,text):\n",
    "            piece = re.search(pattern2,text).group().rstrip(\"本院\")\n",
    "            return piece\n",
    "        elif re.search(pattern3,text):\n",
    "            piece = re.search(pattern3,text).group().rstrip(\"本院\")\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 查找法条\n",
    "    def find_laws(text,pattern1,pattern2):\n",
    "        if re.search(pattern1,text):\n",
    "            pieces = re.findall(pattern1,text)\n",
    "            short_pieces_1 = list(filter(lambda x:len(x) < 50,pieces))\n",
    "            pieces = re.findall(pattern2,text)\n",
    "            short_pieces_2 = list(filter(lambda x:len(x) < 50,pieces))\n",
    "            short_pieces = short_pieces_1 + short_pieces_2\n",
    "            return short_pieces\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    ## 大块文字专用\n",
    "    def find_claims_or_blocks(text,pattern):\n",
    "        if re.search(pattern,text):\n",
    "            piece = re.search(pattern,text).group()\n",
    "            return piece\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    ## 原告诉求\n",
    "    def find_claims(text,pattern1,pattern2):\n",
    "        if re.search(pattern1,text):\n",
    "            piece = re.search(pattern1,text).group()\n",
    "            return piece\n",
    "        elif re.search(pattern2,text):\n",
    "            piece = re.search(pattern2,text).group()\n",
    "            return piece\n",
    "        \n",
    "    ## 重复pattern，构造pattern list\n",
    "    def repeat_pattern(pattern):\n",
    "        pattern_lists = len(list(results)) * [pattern]\n",
    "        return pattern_lists\n",
    "\n",
    "    list_piece_t,list_corpus = [],[] \n",
    "    ### 是否这里可以改成(**kwargs)\n",
    "    ### 原告\n",
    "    \n",
    "    plaintiffs = list(map(find_plaintiffs,corpus_none_n,repeat_pattern(pattern_plaintiff),\n",
    "                          repeat_pattern(pattern_plaintiff_2),repeat_pattern(pattern_plaintiff_3)))\n",
    "\n",
    "    ### 被告\n",
    "    defenders = list(map(find_defender,corpus_none_n,repeat_pattern(pattern_defender),\n",
    "                        repeat_pattern(pattern_defender_2)))\n",
    "    \n",
    "    ### 原告出生年龄\n",
    "    plaintiffs_birthday = list(map(find_plaintiffs_birthday,corpus_none_n,repeat_pattern(pattern_plaintiff_birthday)))\n",
    "\n",
    "    ###被告出生年龄\n",
    "    defenders_birthday = list(map(find_defenders_birthday,corpus_none_n,repeat_pattern(pattern_defender_birthday)))\n",
    "\n",
    "    ### 立案时间\n",
    "    starttimes = list(map(find_starttime,corpus_none_n,repeat_pattern(pattern_starttime)))\n",
    "\n",
    "    ### 诉讼请求\n",
    "    claims = list(map(find_claims,corpus_none_n,repeat_pattern(pattern_claims),repeat_pattern(pattern_claims_2)))\n",
    "\n",
    "    ### 事实与理由\n",
    "    facts = list(map(find_claims_or_blocks,corpus_none_n,repeat_pattern(pattern_facts)))\n",
    "\n",
    "    ### 被告辩称\n",
    "    \n",
    "    defences = list(map(find_defender,corpus_none_n,repeat_pattern(pattern_defence),\n",
    "                        repeat_pattern(pattern_defence_2)))\n",
    "    \n",
    "    ### 法院认定事实\n",
    "    court_facts = list(map(find_court_facts,corpus_none_n,repeat_pattern(pattern_court_fact_1),\n",
    "                          repeat_pattern(pattern_court_fact_2),repeat_pattern(pattern_court_fact_3)))\n",
    "\n",
    "    ### 法院对事实的判定\n",
    "    advises = list(map(find_claims_or_blocks,corpus_none_n,repeat_pattern(pattern_court_advise)))\n",
    "\n",
    "    ### 法院的态度\n",
    "    attitudes = list(map(find_claims_or_blocks,corpus_none_n,repeat_pattern(pattern_court_attitude)))\n",
    "\n",
    "    ### 判决\n",
    "    judgement = list(map(find_claims_or_blocks,corpus_none_n,repeat_pattern(pattern_judgement)))\n",
    "\n",
    "    ### 法条\n",
    "    laws = list(map(find_laws,corpus_none_n,repeat_pattern(pattern_laws_1),\n",
    "                        repeat_pattern(pattern_laws_2)))\n",
    "    table_caipanwenshu = pd.DataFrame([plaintiffs,defenders,plaintiffs_birthday,defenders_birthday,\n",
    "                                   starttimes,claims,facts,defences,court_facts,advises,attitudes,judgement,laws]).T\n",
    "    table_caipanwenshu.columns = [\"原告\",\"被告\",\"原告出生时间\",\"被告出生时间\",\"立案时间\",\"原告诉讼请求\",\n",
    "                              \"原告事实与理由\",\"被告辩称\",\"法院认定事实\",\"法院认为\",\"法院意见\",\"法院判决\",\"法条\"]\n",
    "    return table_caipanwenshu\n",
    "### 判决后：如不服本判决，可以在判决书送达之日起十五日内，向本院递交上诉状，并按照对方当事人或者代表人的人数提出副本，上诉于北京市第一中级人民法院。\n",
    "### 一审，二审要在裁判文书爬虫是就得确定好\n",
    "### 案由，案件类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本清理，剔除掉为空值的单位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 文本清理器2\n",
    "def drop_none_columns(table):\n",
    "    index_exist,index_none = 0,0\n",
    "    list_keep = []\n",
    "    for column in list(table.columns):\n",
    "        for item in table[column]:\n",
    "            if item != None:\n",
    "                index_exist += 1\n",
    "            else:\n",
    "                index_none += 1\n",
    "        ratio = float(index_none) / (float(index_exist) + 1.0) # 剃掉很多都是None的行\n",
    "        if ratio<= 10:\n",
    "            list_keep.append(table[column])\n",
    "        else:\n",
    "            pass\n",
    "        index_exist = 0\n",
    "        index_none = 0\n",
    "    return pd.DataFrame(list_keep).T\n",
    "\n",
    "corpus_n_none = list(filter(lambda x:x != None,corpus))\n",
    "table = clean(corpus_n_none)\n",
    "table_final_1 = table.drop([\"被告\",\"原告出生时间\",\"被告出生时间\",\"立案时间\",\"原告诉讼请求\",\"原告事实与理由\"],axis = 1)\n",
    "table_final_2 = drop_none_columns(table_final_1)\n",
    "table_final_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词频抽取\n",
    "## 原始文本 -> Stopwords后分词 --> 原始关键词词频 完成\n",
    "## 原始文本 -> TextRank关键词提取 --> 关键词抽取 完成  \n",
    "## 原始文本 -> 词性识别 -> 名词抽出 完成\n",
    "## 原始文本 -> TextRank摘要抽取 完成\n",
    "## 原始文本 -> 人名识别 完成\n",
    "## 原始文本 -> 名词共现 -> 语义网络分析 完成\n",
    "\n",
    "# 短语提取\n",
    "## 原始文本 -> 根据关键词推荐文书 完成\n",
    "## 法院认为 -> 搜索\"正当防卫\"，并且re -->得到含有关键词的表述 -- > 人工识别 完成\n",
    "## 法院认为 -> 基于互信息和左右信息熵的短语提取 完成\n",
    "\n",
    "# 词袋模型&分类\n",
    "## 法院判决 -> 使用stopwords筛一遍后 -> TF-IDF变词袋 -> 文档相似度计算 --> 推荐相似的判决 （HanLP可解决）完成\n",
    "## 法院判决 -> 使用stopwords筛一遍后 -> LDA分类 --> 查看分类情况 # 完成\n",
    "## 法院判决 -> 使用stopwords筛一遍后 -> Kmeans文本聚类 --> 查看聚类情况 完成\n",
    "### 法院判决 -> 使用stopwords筛一遍后 -> 循环神经网络 --> 查看分类情况 此步需要有很多次分类与训练，人工标注集\n",
    "\n",
    "# 初始标准分词 范例\n",
    "## HanLP.segment其实是对StandardTokenizer.segment的包装。\n",
    "## print(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))\n",
    "## for term in HanLP.segment(\"你好，欢迎在Python中调用HanLP的API\"):\n",
    "## print('{}\\t{}'.format(term.word, term.nature))\n",
    "\n",
    "# NLP分词 范例\n",
    "## NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "## print(NLPTokenizer.segment(\"我新造一个词叫幻想乡你能识别并正确标注词性吗？\")) \n",
    "## find_all_chinesename(table_final_2[\"原告\"])\n",
    "## table_test_3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入临时文档和上步存储到的表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "#os.chdir(\"正当防卫的1000份文书\")\n",
    "with open(\"corpus.pickle\", 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "table_final_2 = pd.read_excel(\"final_table_2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 法条统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>《中华人民共和国刑法》第二百三十四条</th>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>《中华人民共和国刑事诉讼法》第二百二十五条</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>《中华人民共和国民事诉讼法》第二百五十三条</th>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>《中华人民共和国民事诉讼法》第一百七十条</th>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>《中华人民共和国侵权责任法》第六条</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        数量\n",
       "《中华人民共和国刑法》第二百三十四条     391\n",
       "《中华人民共和国刑事诉讼法》第二百二十五条  204\n",
       "《中华人民共和国民事诉讼法》第二百五十三条  155\n",
       "《中华人民共和国民事诉讼法》第一百七十条   130\n",
       "《中华人民共和国侵权责任法》第六条      120"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 法条统计\n",
    "def count_laws(table):\n",
    "    \"\"\"\n",
    "    param table:所有法条文本的列表\n",
    "    return table_laws_ascending:文本\n",
    "    \"\"\"\n",
    "    list_fatiao_corpus = []\n",
    "    fatiao2d = list(table[\"法条\"])\n",
    "    for case in fatiao2d:\n",
    "        if isinstance(case,str):\n",
    "            case = (eval(case))\n",
    "            case_unrepeated = list(set(case))\n",
    "            for law in case_unrepeated:\n",
    "                list_fatiao_corpus.append(law)\n",
    "    table_laws = pd.DataFrame([dict(Counter(list_fatiao_corpus))]).T\n",
    "    table_laws.columns = [\"数量\"]\n",
    "    table_laws_ascending = table_laws.sort_values(\"数量\",ascending = False)\n",
    "    return table_laws_ascending\n",
    "\n",
    "count_laws(table_final_2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([list_key_expressions]).T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绝对词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# 绝对词频统计\n",
    "from pyhanlp import *\n",
    "import jpype\n",
    "\n",
    "pattern_word = re.compile(r\".*?\\\\\")\n",
    "pattern_nature = re.compile(r\"\\\\.*\")\n",
    "\n",
    "\n",
    "list_useful_words,list_natures = [],[]\n",
    "def segments(article,stopwords):\n",
    "    for item in HanLP.segment(article):\n",
    "        try:\n",
    "            item_word = ('{}\\{}'.format(item.word, item.nature))\n",
    "            word = re.search(pattern_word,item_word).group()\n",
    "            nature = re.search(pattern_nature,item_word).group()\n",
    "            word_clean = word.strip(\"\\\\\")\n",
    "            nature_clean = nature.strip(\"\\\\\")\n",
    "            if word_clean not in list_stopwords:\n",
    "                list_useful_words.append(word_clean)\n",
    "                list_natures.append(nature_clean)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return list_useful_words,list_natures\n",
    "\n",
    "def DF(list_useful_words,list_natures):\n",
    "    table_test = pd.DataFrame([list_useful_words,list_natures]).T\n",
    "    table_test.columns = [\"word\",\"nature\"]\n",
    "    table_test_2 = table_test[table_test[\"nature\"].isin([\"n\",\"v\"])]\n",
    "    table_test_3 = table_test_2.groupby(\"word\").count().sort_values([\"nature\"],ascending = [\"False\"])\n",
    "    return table_test_3\n",
    "\n",
    "def main_word_freqs(corpus,list_stopwords):\n",
    "    list_articles = []\n",
    "    for article in corpus[0:10]:\n",
    "        try:\n",
    "            list_useful_words,list_natures = segments(article,list_stopwords)\n",
    "            table = DF(list_useful_words,list_natures)\n",
    "            list_articles.append(table)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return list_articles\n",
    "\n",
    "\n",
    "def NLP_segments_method():\n",
    "    pass\n",
    "\n",
    "list_stopwords = []\n",
    "with open(\"stopwords.txt\",\"r\",encoding = \"utf-8\") as f:\n",
    "    for word in f.readlines():\n",
    "        list_stopwords.append(word.strip(\"\\n\"))\n",
    "        \n",
    "corpus = list(filter(lambda x:x!=None,corpus))\n",
    "list_articles = main_word_freqs(corpus,list_stopwords)\n",
    "# pd.concat(list_articles).groupby(\"word\").sum().sort_values(\"nature\",ascending = False) 总词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -*- coding:utf-8 -*-\n",
    "## 关键词提取\n",
    "list_keywords = []\n",
    "TextRankKeyword = JClass(\"com.hankcs.hanlp.summary.TextRankKeyword\")\n",
    "def segments_keywords(article):\n",
    "    keywords = list(HanLP.extractKeyword(article,10))\n",
    "    list_keywords.append(keywords)\n",
    "    \n",
    "for article in corpus[0:5]:\n",
    "    segments_keywords(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本摘要抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 摘要\n",
    "TextRankSentence = JClass(\"com.hankcs.hanlp.summary.TextRankSentence\")\n",
    "corpus = list(filter(lambda x:x!=None,corpus))\n",
    "list_summary = []\n",
    "def summary(article):\n",
    "    sentence_list = list(HanLP.extractSummary(article,10))\n",
    "    list_summary.append(sentence_list)\n",
    "    \n",
    "for article in corpus[0:5]:\n",
    "    summ = summary(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据关键词推荐文书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "## 根据关键词推荐文书\n",
    "Suggester = JClass(\"com.hankcs.hanlp.suggest.Suggester\")\n",
    "suggester = Suggester()\n",
    "for article in corpus[0:5]:\n",
    "    suggester.addSentence(article)\n",
    "    \n",
    "list(suggester.suggest(\"赔偿\",1))[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "## 根据关键词推荐结果\n",
    "Suggester = JClass(\"com.hankcs.hanlp.suggest.Suggester\")\n",
    "suggester = Suggester()\n",
    "results = list(filter(lambda x:isinstance(x,str),final_table_2[\"法院判决\"]))\n",
    "\n",
    "for article in results[0:5]:\n",
    "    suggester.addSentence(article)\n",
    "\n",
    "list_suggestions = list(suggester.suggest(\"驳回\",100))\n",
    "list_suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取文本中实体短语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "## 提取短语\n",
    "\"\"\"\n",
    "for text in results[0:10]:\n",
    "    phrase_list = HanLP.extractPhrase(text,5)\n",
    "    print(text)\n",
    "    print(phrase_list)\n",
    "\"\"\"\n",
    "results_clean = list(map(lambda x:x.strip(\"判决如下:\"),results))\n",
    "a1 = find_keywords_in_courts_v1(results_clean)\n",
    "a2 = find_keywords_in_courts_v2(results_clean)\n",
    "a3 = find_keywords_in_courts_v3(results_clean)\n",
    "\n",
    "for item in list(map(lambda x:\"\".join(x),a2)):\n",
    "    phrase_list = HanLP.extractPhrase(item,5)\n",
    "    print(phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_clean[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词语共现、语义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -*- coding:utf-8 -*-\n",
    "## 词语共现模块\n",
    "\n",
    "## 将第一步所得到的词频表合并 \n",
    "table_topwords = pd.concat(list_articles).groupby(\"word\").sum().sort_values(\"nature\",ascending = False)\n",
    "\n",
    "## 重命名表头\n",
    "table_topwords[\"词语\"] = table_topwords.index\n",
    "table_topwords.index = range(len(table_topwords))\n",
    "table_topwords.columns = [\"频率\",\"词语\"]\n",
    "\n",
    "## 共现第一步，找每篇文章里存在多少词\n",
    "def find_word_in_articles(corpus,word_table):\n",
    "    \"\"\"\n",
    "    word_table:高频词列表，此处设定为：table_topwords[\"词语\"]\n",
    "    corpus:文档文集，此处设定为：corpus[0:10],只取前十个做范例\n",
    "    \"\"\"\n",
    "    list_vector = []\n",
    "    list_matrix = []\n",
    "    for item in word_table:\n",
    "        for word_list in [sentence for sentence in corpus]:\n",
    "            if item in str(word_list):\n",
    "                list_vector.append(1)\n",
    "            else:\n",
    "                list_vector.append(0)\n",
    "        list_matrix.append(list_vector)\n",
    "        list_vector = []\n",
    "    return list_matrix\n",
    "    \n",
    "## 共现第二步，找词语共现情况\n",
    "def find_coappear(word1,word2):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    word_table:高频词列表，此处设定为：table_topwords[\"词语\"]\n",
    "    corpus:文档文集，此处设定为：corpus[0:10],只取前十个做范例\n",
    "    \"\"\"\n",
    "    coappear = 0\n",
    "    for item in zip(table_matrix.T[word1],table_matrix.T[word2]):\n",
    "        if item == (1,1):\n",
    "            coappear += 1\n",
    "    list_vector.append(coappear)\n",
    "    list_matrix.append(list_vector)\n",
    "\n",
    "#list_matrix_pre = find_word_in_articles(corpus[0:10],table_topwords[\"词语\"])\n",
    "table_matrix = pd.DataFrame(list_matrix_pre)\n",
    "table_matrix.index = table_topwords[\"词语\"]\n",
    "print(table_matrix.head())\n",
    "\n",
    "list_vector = []\n",
    "list_matrix = []    \n",
    "table_matrix = table_matrix[0:100]\n",
    "for word1 in table_matrix.index:\n",
    "    list_vector = []\n",
    "    for word2 in table_matrix.index:\n",
    "        find_coappear(word1,word2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -*- coding:utf-8 -*-\n",
    "\n",
    "## 共现第三步，发现最大共现\n",
    "table_coappear = pd.DataFrame(list_matrix)[0::100]\n",
    "table_coappear.index = table_topwords[0:100][\"词语\"]\n",
    "table_coappear.columns = table_topwords[0:100][\"词语\"]\n",
    "table_co.index = table_co[\"词语\"]\n",
    "list_save_tuple = []\n",
    "for item in table_co.index:\n",
    "    for index,value in zip(table_co[item].index,table_co[item]):\n",
    "        list_save_tuple.append((item,index,value))\n",
    "table_co2 = pd.DataFrame(list_save_tuple)\n",
    "table_co2.columns = [\"word1\",\"word2\",\"weight\"]\n",
    "list_unrepeated = []\n",
    "for i,j,q in zip(table_co2[\"word1\"],table_co2[\"word2\"],table_co2[\"weight\"]):\n",
    "    if i != j:\n",
    "        list_unrepeated.append((i,j,q))\n",
    "table_unrepeated = pd.DataFrame(list_unrepeated)\n",
    "table_unrepeated.columns = [\"word1\",\"word2\",\"weight\"]\n",
    "table_unrepeated_s = table_unrepeated.sort_values(\"weight\",ascending = False)\n",
    "table_unrepeated_s\n",
    "\n",
    "## 第四步，扔到gephi里，进一步发现语义网络图谱\n",
    "table_unrepeated_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询含有关键词的上下文和句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "## 查询含有关键词的上下文\n",
    "import re\n",
    "\n",
    "# “正当防卫”查询\n",
    "def find_keywords_in_courts_v1(li):\n",
    "    \"\"\"\n",
    "    param li:含有“正当防卫”（或其他关键词）的文本结果列表\n",
    "    return list_key_expressions:所有含有关键词的上下文\n",
    "    \"\"\"\n",
    "    list_key_expressions = []\n",
    "    pattern_sentences = re.compile(\"([^。]*正当防卫[^。]*)。\")\n",
    "    for item in li:\n",
    "        try:\n",
    "            facts = re.findall(pattern_sentences,item)\n",
    "            list_key_expressions.append(facts)\n",
    "        except Exception as e:\n",
    "            facts = None\n",
    "            list_key_expressions.append(None)\n",
    "    return list_key_expressions\n",
    "\n",
    "# “判处”查询\n",
    "def find_keywords_in_courts_v2(li):\n",
    "    list_key_expressions = []\n",
    "    pattern_sentences = re.compile(\"([^。]*判处[^。]*)。\")\n",
    "    for item in li:\n",
    "        try:\n",
    "            facts = re.findall(pattern_sentences,item)\n",
    "            list_key_expressions.append(facts)\n",
    "        except Exception as e:\n",
    "            facts = None\n",
    "            list_key_expressions.append(None)\n",
    "    return list_key_expressions\n",
    "\n",
    "def find_keywords_in_courts_v3(li):\n",
    "    list_key_expressions = []\n",
    "    pattern_sentences = re.compile(\"([^。]*犯.*?罪[^。]*)。\")\n",
    "    for item in li:\n",
    "        try:\n",
    "            facts = re.findall(pattern_sentences,item)\n",
    "            list_key_expressions.append(facts)\n",
    "        except Exception as e:\n",
    "            facts = None\n",
    "            list_key_expressions.append(None)\n",
    "    return list_key_expressions\n",
    "\n",
    "list_key_expression = find_keywords_in_courts_v1(table_final_2[\"法院认为\"])\n",
    "list_key_expression_clean = list(filter(lambda x:x != None,list_key_expression))\n",
    "list_key_expression_list = []\n",
    "for item in list_key_expression_clean:\n",
    "    if item != []:\n",
    "        list_key_expression_list.append(item)\n",
    "pd.DataFrame(list_key_expression_list).to_excel(\"table_zhengdang.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文人名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 中文人名识别\n",
    "def find_all_chinesename(corpus_list):\n",
    "    \"\"\"\n",
    "    param corpus_list:目标文本的列表\n",
    "    return names_extract:名字列表\n",
    "    \"\"\"\n",
    "    names_extract = []\n",
    "    segment = HanLP.newSegment().enableNameRecognize(True)\n",
    "    for sentence in li:\n",
    "        try:\n",
    "            term_list = segment.seg(sentence)\n",
    "            for item in term_list:\n",
    "                if \"/nr\" in str(item):\n",
    "                    names_extract.append(str(item).strip(\"/nr\"))\n",
    "                elif \"/nz\" in str(item):\n",
    "                    names_extract.append(str(item).strip(\"/nz\"))\n",
    "                else:\n",
    "                    names_extract.append(None)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return names_extract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 地名识别&实体机构名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法律联盟\n",
      "云南省昆明市中级人民法院\n",
      "法律联盟\n",
      "中级人民法院\n",
      "中级人民法院\n",
      "保定市中级人民法院\n",
      "中级人民法院\n",
      "中级人民法院\n",
      "法律联盟\n",
      "所居住社区\n",
      "法律联盟\n",
      "内蒙古自治区高级人民法院\n",
      "指二人\n",
      "中级人民法院\n",
      "法律联盟\n",
      "所居住社区\n",
      "楚雄彝族自治州\n",
      "大理白族自治州\n",
      "中华人民共和国\n",
      "河北省\n",
      "保定市\n",
      "沧州市\n",
      "河北省\n",
      "廊坊市\n",
      "中华人民共和国\n",
      "社区\n",
      "中华人民共和国\n",
      "鄂尔多斯市\n",
      "中华人民共和国\n",
      "社区\n"
     ]
    }
   ],
   "source": [
    "segment_organization = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "segment_place = HanLP.newSegment().enablePlaceRecognize(True)\n",
    "\n",
    "def find_specific_word(segment_method,corpus,limitation):\n",
    "    \"\"\"\n",
    "    搜索机构名/地名\n",
    "    param segment_method:查询地名，还是机构名\n",
    "    param corpus:查询怎样的文本\n",
    "    param limitation:限定词词性\n",
    "    \"\"\"\n",
    "    for sentence in corpus:\n",
    "        term_list = segment_method.seg(sentence)\n",
    "        for every_word in term_list:\n",
    "            if limitation in str(every_word):\n",
    "                if \"之日起\" not in str(every_word):\n",
    "                    print(str(every_word).strip(\"/nt/ns\"))\n",
    "                    \n",
    "results = list(filter(lambda x:isinstance(x,str),table_final_2[\"法院判决\"]))\n",
    "find_specific_word(segment_organization,results[0:10],\"/nt\")\n",
    "find_specific_word(segment_place,results[0:10],\"/ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 三种分类：LDA,K-means文本聚类,神经网络\n",
    "## HanLP 文本聚类方法\n",
    "## https://github.com/hankcs/pyhanlp/blob/4c1a5da4f00877600810c6b34431fc1755b75242/tests/demos/demo_text_clustering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "#os.chdir(\"正当防卫的1000份文书\")\n",
    "with open(\"corpus.pickle\", 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "table_final_2 = pd.read_excel(\"final_table_2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立TF-IDF，使用余弦相似度来查找文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF-IDF -- 使用余弦相似度查找文本\n",
    "### 此处使用jieba，不使用HanLP\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "import codecs, sys\n",
    "import jieba\n",
    "\n",
    "def clean(single_para):\n",
    "    \"\"\"\n",
    "    将stopwords和每一段文本取差集，清洗数据\n",
    "    param single_para:没清洗文本集中的每一段数据\n",
    "    return list_clean:每一段清洗好的文本\n",
    "    \"\"\"\n",
    "    list_clean = set(single_para).difference(set(stopwords_clean))\n",
    "    return list_clean\n",
    "\n",
    "def data_prepare(text_list):\n",
    "    \"\"\"\n",
    "    把数据清理干净\n",
    "    param text_list:没清洗过的文本列表\n",
    "    return list_clean:清洗过的文本列表\n",
    "    \"\"\"\n",
    "    table_segments_list = list(map(lambda x:jieba.lcut(x),table_judgements))\n",
    "    stopwords = list(map(lambda x:x.strip(\"\\n\"),codecs.open('stopwords.txt', 'r', 'utf-8').readlines()))\n",
    "    stopwords_clean = list(map(lambda x:x.strip(\"\\r\"),stopwords))\n",
    "    list_clean = list(map(clean,table_segments_list))\n",
    "    return list_clean\n",
    "\n",
    "def tfidf(clean_text):\n",
    "    \"\"\"\n",
    "    产生tf-idf的模型\n",
    "    param clean_text:清洗过的文本列表\n",
    "    return tf-idf模型:\n",
    "    \"\"\"\n",
    "    texts = list_clean\n",
    "    # 建立词典\n",
    "    dictionary = corpora.Dictionary(list_clean)\n",
    "\n",
    "    # 存档词典\n",
    "    dictionary.save('dict_v1.dict')\n",
    "\n",
    "    # 建立词袋模型\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # 建立tf-idf模型\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "    # 将词袋模型，转换为tf-idf模型\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf\n",
    "\n",
    "def find_similar_result(corpus_tfidf,article_arg):\n",
    "    \"\"\"\n",
    "    使用tf-idf模型，计算文本间的余弦相似，得到结果、\n",
    "    param corpus_tfidf:tf-idf模型\n",
    "    param article_arg:目标结果的下标号\n",
    "    return list_final_similarities:最终的相似结果\n",
    "    \"\"\"\n",
    "    # 创建索引\n",
    "    index = similarities.MatrixSimilarity(corpus_tfidf)\n",
    "    list_final_similarities = []\n",
    "    \n",
    "    # 查找最相似的十个判决结果,以第一个文本为例\n",
    "    sims = index[corpus_tfidf[0]]\n",
    "    args = np.argsort(sims)[::-1][1:10]\n",
    "    for item in list(args):\n",
    "        list_final_result.append(list(table_judgements)[item])\n",
    "    return list_final_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda 分类部分\n",
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5, iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(corpus_tfidf,table_original_texts,num_topics):\n",
    "    \"\"\"\n",
    "    使用lda算法进行分类\n",
    "    param corpus_tfidf:tf-idf模型\n",
    "    param table_original_texts:原始文本的列表\n",
    "    param num_topics:分类多少个主题\n",
    "    return table_keyword_classfication:关键词和关键词的分类表\n",
    "    return table_originaltext_classfication:原文文档和关键词的分类表\n",
    "    \"\"\"\n",
    "    # lda 分类部分\n",
    "    lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5, iterations=500)\n",
    "\n",
    "    # 关键词抽取 - 以及关键词属于哪一类\n",
    "    top_words_per_topic = []\n",
    "    for t in range(lda.num_topics):\n",
    "        top_words_per_topic.extend([(t, ) + x for x in lda.show_topic(t, topn = 10)])\n",
    "    table_keyword_classfication = pd.DataFrame(top_words_per_topic).head()\n",
    "    table_keyword_classfication.to_excel(\"keyword_classification.xlsx\")\n",
    "    \n",
    "    # 原始文章的归类\n",
    "    list_classification_f = []\n",
    "    for x in [item for item in lda.get_document_topics(corpus)]:\n",
    "        list_possibility = list(map(lambda x:x[1],x))\n",
    "        list_classification = list(map(lambda x:x[0],x))\n",
    "        list_possibility_index = list_possibility.index(max(list_possibility))\n",
    "        result = list_classification[list_possibility_index]\n",
    "        list_classification_f.append(result)\n",
    "    table_originaltext_classfication = pd.DataFrame([list(table_judgements),list_classification_f]).T.head()\n",
    "    table_originaltext_classfication.to_excel(\"keyword_classification.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_judgements = table_final_2[\"法院判决\"].dropna()\n",
    "list(table_judgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf建模后，使用K-means聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf-idf建模后K-means聚类\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def clean(single_para,stopwords_clean):\n",
    "    \"\"\"\n",
    "    将stopwords和每一段文本取差集，清洗数据\n",
    "    param single_para:没清洗文本集中的每一段数据\n",
    "    return list_clean:每一段清洗好的文本\n",
    "    \"\"\"\n",
    "    list_clean = set(single_para).difference(set(stopwords_clean))\n",
    "    return list_clean\n",
    "\n",
    "def data_prepare(text_list):\n",
    "    \"\"\"\n",
    "    把数据清理干净\n",
    "    param text_list:没清洗过的文本列表\n",
    "    return list_clean:清洗过的文本列表\n",
    "    \"\"\"\n",
    "    table_segments_list = list(map(lambda x:jieba.lcut(x),text_list))\n",
    "    stopwords = list(map(lambda x:x.strip(\"\\n\"),codecs.open('stopwords.txt', 'r', 'utf-8').readlines()))\n",
    "    stopwords_clean = list(map(lambda x:x.strip(\"\\r\"),stopwords))\n",
    "    list_clean = list(map(clean,table_segments_list,stopwords_clean))\n",
    "    return list_clean\n",
    "\n",
    "def tf_idf_scikit(words):\n",
    "    \"\"\"\n",
    "    使用scikit-learn建立tf-idf模型\n",
    "    param words:清洗过的文本列表\n",
    "    return tf-idf:tf-idf词袋模型\n",
    "    \"\"\"\n",
    "    \n",
    "    # 将文本中的词语转换为词频矩阵 矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # 统计每个词语的tf-idf权值\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    # 第一个fit_transform是计算tf-idf 第二个fit_transform是将文本转为词频矩阵\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(words))\n",
    "\n",
    "    # 获取词袋模型中的所有词语\n",
    "    word = vectorizer.get_feature_names()\n",
    "\n",
    "    # 将tf-idf矩阵抽取出来，元素w[i][j]表示j词在i类文本中的tf-idf权重\n",
    "    weight = tfidf.toarray()\n",
    "    return weight\n",
    "\n",
    "def classifier(weight,clusters_num):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    list_classification = []\n",
    "    ## 设计随机种子\n",
    "    random_status = random.randint(1,1000)\n",
    "\n",
    "    ## 建立Kmeans分类器\n",
    "    clf = KMeans(init='k-means++',n_clusters = clusters_num,random_state = random_status,n_init=10)\n",
    "\n",
    "    ## PCA降维处理\n",
    "    reduced_data = pca(weight)\n",
    "\n",
    "    ## 训练\n",
    "    s = clf.fit(reduced_data)\n",
    "    \n",
    "    ## 参数显示\n",
    "    # print(np.shape(clf.cluster_centers_)) #簇心\n",
    "    print(\"标签数量\",clf.labels_) \n",
    "    print(\"簇心之间的平均欧氏距离\",clf.inertia_) #簇心之间的欧氏距离\n",
    "\n",
    "    ## 文章归类处理\n",
    "    i = 1\n",
    "    while i <= len(clf.labels_):\n",
    "        list_classification.append(clf.labels_[i-1])\n",
    "        i = i + 1\n",
    "    return list_classification\n",
    "\n",
    "def pca(weights):\n",
    "    reduced_data = PCA(n_components=2).fit_transform(weights)\n",
    "    return reduced_data\n",
    "\n",
    "def ploting(reduced_data): \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = clf.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "              'Centroids are marked with white cross')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n",
    "\n",
    "## 得到判决表\n",
    "def data_input(table):\n",
    "    \"\"\"\n",
    "    引入文本材料\n",
    "    Param table:目标列表\n",
    "    return table_judgements:含有目标文本的list\n",
    "    \"\"\"\n",
    "    table_judgements = table[\"法院判决\"].dropna()\n",
    "    return table_judgements\n",
    "\n",
    "table_judgements = data_input(table_final_2)\n",
    "\n",
    "## 使用一百篇篇文章来做测试\n",
    "data_test = data_prepare(list(table_judgements))[0:100] \n",
    "\n",
    "## 单词列表组成字符串\n",
    "words = list(map(lambda x:\" \".join(list(x)),data_test))\n",
    "\n",
    "## tf-idf转换weight\n",
    "weight = tf_idf_scikit(words)\n",
    "\n",
    "## 分类器得到分类\n",
    "list_classification = classifier(weight,4)\n",
    "\n",
    "## pca降维\n",
    "reduced_data = pca(weight)\n",
    "\n",
    "## 将分类情况可视化\n",
    "ploting(reduced_data)\n",
    "\n",
    "## 将分类结果形成列表\n",
    "pd.DataFrame([list(table_judgements[0:100]),list_classification])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用神经网络分类（RNN，初步分类）\n",
    "* 该方法只适用于判断某一类文本\n",
    "* 回答如“是否该文本里，法院支不支持正当防卫”诸如此类的问题\n",
    "* 但这个方法需要使用大量人工标注的文本，因此需要等大家做完很多题目以后，进行神经网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用正当防卫1000个案例为例，来执行神经网络\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(\".\")\n",
    "os.chdir(\"word2vec-nlp-tutorial\")\n",
    "data_t = pd.read_csv(\"labeledTrainData.tsv\",sep = \"\\t\")\n",
    "del data_t[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练word_vec模型\n",
    "if not os.path.exists(\"mymodel\"):\n",
    "    if not os.path.exists(\"imdb_text\"):\n",
    "        data_un = pd.read_csv(\"unlabeledTrainData.tsv\",\n",
    "                             headers = 0,\n",
    "                             delimiter = \"\\t\",\n",
    "                             quoting = 3)\n",
    "        pat = re.compile(r\"[A-Za-z]+|[!?,:().]\")\n",
    "        with open(\"imdb_text\",\"a\",encoding = \"utf-8\") as f:\n",
    "            for rev in data_un.review:\n",
    "                str_list = pat.findall(rev)\n",
    "                str_list = [x.lower() for x in str_list]\n",
    "                string = \" \".join(str_list)\n",
    "                f.write(string + \"\\n\")\n",
    "            del data_un\n",
    "    sentences = word2vec.Text8Corpus(\"imdb_text\")\n",
    "    model = word2vec.Word2Vec(sentences,size = 50)\n",
    "    \n",
    "    # 训练skip-gram模型，默认window=5\n",
    "    model.save(\"mymodel\")\n",
    "else:\n",
    "    model = word2vec.Word2Vec.load(\"mymodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "del model\n",
    "data_t[\"vec\"] = data_t.review.apply(lambda x:[word_vectors[w] for w in x.split() if w in word_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_t.head().vec[0])\n",
    "del data_t['review']\n",
    "del word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = data_t[data_t[\"vec\"].apply(lambda x:len(x) > 0)]\n",
    "data_t.sentiment.value_counts()\n",
    "maxlength = max([len(x) for x in data_t.vec])\n",
    "maxlength\n",
    "# 评价为1，正面的12499\n",
    "# 负面为0，负面的12495\n",
    "# 有几个没有被训练出词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data_t.vec.apply(len) > 300)\n",
    "## 填充评论长度\n",
    "def pad(x):\n",
    "    if len(x) > 300:\n",
    "        x1 = x[:300] # 超过300的，截取0：300的\n",
    "    else:\n",
    "        x1 = np.zeros((300,50))\n",
    "        x1[:len(x)] = x # 不满300的，就直接填充0\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t[\"vec\"] = data_t.vec.apply(pad)\n",
    "data_t.vec.head()[0]\n",
    "np.shape(data_t.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN部分\n",
    "import tensorflow as tf\n",
    "learning_rate = 0.002\n",
    "batch_size = 100\n",
    "n_input = 50\n",
    "n_steps = 300\n",
    "n_hidden = 300\n",
    "n_classes = 2\n",
    "\n",
    "# RNN的输入维度，为3个维度，第一维度为输入的批次\n",
    "# 第二维度为输入step，word-vector的长度，或者说文本的顺序\n",
    "# 第三维度为输入单词向量，每个单词以50个向量来表示\n",
    "# [批次,step,input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,n_steps,n_input])\n",
    "y = tf.placeholder(tf.int64,[None])\n",
    "keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(shuru):\n",
    "    return tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(shuru),reduction_indices = 2)),reduction_indices = 1)\n",
    "# 找出真正长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(n_hidden,\n",
    "                                    kernel_initializer = tf.truncated_normal_initializer(stddev = 0.0001),\n",
    "                                    bias_initializer = tf.truncated_normal_initializer(stddev = 0.0001)),\n",
    "                                    output_keep_prob = keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output,_ = tf.nn.dynamic_rnn(\n",
    "           cell,\n",
    "           x,\n",
    "           dtype = tf.float32,\n",
    "           sequence_length = length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.get_shape()\n",
    "index = tf.range(0,batch_size) * n_steps + (tf.cast(length(x),tf.int32) - 1)\n",
    "flat = tf.reshape(output,[-1,int(output.get_shape()[2])])\n",
    "last = tf.gather(flat,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal((n_hidden,n_classes),stddev = 0.001))\n",
    "bias = tf.Variable(tf.constant(0.1,shape = [n_classes]))\n",
    "com_out = tf.matmul(last,weight) + bias\n",
    "prediction = tf.nn.softmax(com_out) ## 用来计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = com_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads = optimizer.compute_gradients(cross_entropy) # 梯度裁剪\n",
    "for i,(g,v) in enumerate(grads):\n",
    "    if g is not None:\n",
    "        grads[i] = (tf.clip_by_norm(g,5),v)\n",
    "train_op = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(prediction,1),y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 产生批次\n",
    "def generatebatch(X,Y,n_examples,batch_size):\n",
    "    for batch_i in range(n_examples // batch_size):\n",
    "        start = batch_i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_xs = X[start:end]\n",
    "        batch_ys = Y[start:end]\n",
    "        yield batch_xs,batch_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 0\n",
    "for step in range(10):\n",
    "    index = np.random.permutation(int(len(data_t.vec.values)))\n",
    "    for batch_x,batch_y in generatebatch(data_t.vec.values[index],data_t.sentiment.values[index],len(data_t.vec.values),batch_size):\n",
    "        batch_x = np.concatenate(batch_x).reshape(batch_size,300,50)\n",
    "        batch_x.astype(np.float64)\n",
    "        sess.run(train_op,feed_dict = {x:batch_x,y:batch_y,keep_prob:0.5})\n",
    "        loss = sess.run(cross_entropy,feed_dict = {x:batch_x,y:batch_y,keep_prob:1})\n",
    "        print(loss)\n",
    "    acc = sess.run(accuracy,feed_dict = {x:batch_x,y:batch_y,keep_prob:1})\n",
    "    loss = sess.run(cross_entropy,feed_dict = {x:batch_x,y:batch_y,keep_prob:1})\n",
    "    #saver.save(sess,'./teansorflow',global_step = step)\n",
    "    print(\"Iter \" + str(step) + \",Minibatch Loss= \".format(loss) + \"{}\".format(acc))\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "386.997px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
